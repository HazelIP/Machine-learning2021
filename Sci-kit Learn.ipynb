{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542b1184-170c-4956-92c2-dc223fb9a7f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sci-kit Learn\n",
    "## link:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120287ba-95b9-4238-a8e4-0c142b52fdfe",
   "metadata": {},
   "source": [
    "What is Sci-kit Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934aed75-029f-4cf1-9796-ecb105daaaa5",
   "metadata": {},
   "source": [
    "## Overview of sci-kit Learn\n",
    "Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language. It was initially developed by David Cournapeau in 2007. It features various classification, regression and clustering algorithms and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. (wiki: https://en.wikipedia.org/wiki/Scikit-learn) <br>\n",
    "- built upon SciPy\n",
    "- include NumPy, Matplotlib, Pandas\n",
    "- focus on modeling data, while NumPy, Pandas focus on loading, manipulating and summarizing data [C1] <br/>\n",
    "\n",
    "1. Classification\n",
    "2. Regression\n",
    "3. Clustering\n",
    "4. Dimensionality reduction\n",
    "5. Model selection\n",
    "6. Preprocessing <br/>\n",
    "(from scikit-learn.org)\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008396ce-4378-4711-bb2d-7a6f9ba16297",
   "metadata": {},
   "source": [
    "**How does machine learning works?**\n",
    "- supervised learning\n",
    "- unsupervise learning\n",
    "- a data set is divided into training set and testing set, then evaluate its predictability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21b11c-6120-415b-b018-3b94e0d792e2",
   "metadata": {},
   "source": [
    "I. Supervised learning: learn from sameple data to predict with new data\n",
    "*  Classification: discrete form, a limited number of categories. aim to label them with correct category or class\n",
    "*  Regression: continuous variables, e.g. prediction of salmon's length as a function of its age and weight\n",
    "\n",
    "II. Unsupervised learning: consists sets of input without corresponding target values \n",
    "*   Clustering: aim to discover groups of similar examples within the data\n",
    "*   Density estimation: aim to determine the distribution of data within the sample OR project data from high-dimensional space down to 2/3D for the purpose of visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fedee6f-d932-4fbd-a9e8-b0428ba4378c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5863af-27b0-429d-9645-5cd2c78dcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5d208-2022-406e-8577-37893f74cd54",
   "metadata": {},
   "source": [
    "**What algorithms I might like to include in assessment?**\n",
    "1. Decision Tree (explanation)/ gini impurity/ID3\n",
    "2. Random Forest (prediction)/bootstrap aggregating/bagging\n",
    "3. pruning algorithms/information gain/minimal cost-complexity pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca7a41-131c-4f94-aee7-f9a9abdd2852",
   "metadata": {},
   "source": [
    "### How decision tree classifier works?\n",
    "https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bdc61-fb89-45e9-b3ff-b3c58cdbb89b",
   "metadata": {},
   "source": [
    "It is under the umbrella of **Supervised Learning** algorithm, which is mainly used for classification problem. It works for both categorical **(Classification tree)** and continuous **(Regression tree)** dependent variables. It splits the population into two or more heterogeneous groups using various techniques like Gini index, Information gain, entropy. As well as identifying the most significant variable. <br>\n",
    "It can also be used in data exploration stage. For example, we are working on a problem where we have information available in hundreds of variables, there decision tree will help to identify most significant variable.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57138c20-086c-45fa-a191-602e07f08abc",
   "metadata": {},
   "source": [
    "#### How does a tree based algorithms decide where to split\n",
    "The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees.\n",
    "\n",
    "Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n",
    "\n",
    "The algorithm selection is also based on type of target variables. Let’s look at the four most commonly used algorithms in decision tree:\n",
    "1. Gini\n",
    "2. Chi-square\n",
    "3. Information gain\n",
    "4. Reduction in Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4753b-c2bf-4afa-9a3a-1a8c7d225880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40229fa1-9cd4-42ed-b04f-c2c23e8d8371",
   "metadata": {},
   "source": [
    "### Gini Impurity (decision tree) (supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d0f95-9daf-4823-af95-2e680ec8a1ab",
   "metadata": {},
   "source": [
    "\n",
    "Random forest may be a flexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning, an excellent result most of the time.\n",
    "\n",
    "It’s also one among the foremost used algorithms, due to its simplicity and variety (it are often used for both classification and regression tasks).\n",
    "\n",
    "Random Forests are an ensemble learning method that is for performing classification, regression as well as other tasks through the construction of decision trees and providing the output as a class which is the mode or mean of the underlying individual trees.\n",
    "\n",
    "A Decision Tree Classifier functions by breaking down a dataset into smaller and smaller subsets based on different criteria. Different sorting criteria will be used to divide the dataset, with the number of examples getting smaller with every division.\n",
    "\n",
    "Once the network has divided the data down to one example, the example will be put into a class that corresponds to a key. When multiple random forest classifiers are linked together they are called Random Forest Classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05be674-f014-4810-a13e-03d804a8e34c",
   "metadata": {},
   "source": [
    "**Classification Tree**\n",
    "- make tree for each variable, check and compare impurity among variables (Gini impurity, imformation gain, entropy)\n",
    "- Total Gini impurity = weighted average of gini impurities for the leaves\n",
    "- a bit more calculation if variable is numeric\n",
    "- variable with lowest impurity at the top of the tree, then calcualtion of Gini Imppurity starts again for the remaining variables as nodes, until no more split --> as leaf and categorize/classify them\n",
    "**confidence in classification and overfit**\n",
    "- Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa41bd-9e0b-41ca-b80d-73cb7a4a2a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84badc65-3f64-4625-a167-af798fd070d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965a80e-7e7f-4d77-a4cf-c84b50236f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddb54df-5532-475d-886d-ca0677c850bc",
   "metadata": {},
   "source": [
    "### Bootstrap aggregating/baggin algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b32f2-6a5f-450a-af62-e059aa581379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a7f7d-ea6c-44d9-bb5b-cfe2e2f8a66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a465bc2-5b9a-4496-8a00-0f42bd09243d",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "https://builtin.com/data-science/random-forest-algorithm\n",
    "\n",
    "https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "\n",
    "https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/\n",
    "\n",
    "\n",
    "It is essentially an ensemble of decision trees. While decision trees are better at classification, random forest is better at prediction. \n",
    "multiple decision trees, usually trained with the \"bagging\" method which is generally combination learning models increases the overall result in terms of accuracy and stability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f5e52-0399-41ec-aa86-2a039fe7f983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c01237df-0361-4f55-ad47-dab6589ae007",
   "metadata": {},
   "source": [
    "### Pruning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f14349-6c9d-43f6-b25a-dc4f232a5d52",
   "metadata": {},
   "source": [
    "#### random forest\n",
    "- because DT is easy to create but inaccurate to classify\n",
    "- bootstrapped dataset; consider a random subset of variables at each step; then repeat --> a wide variety of trees (the forest)\n",
    "- prediction, run data on all the trees in the forest, see which option received more votes\n",
    "- bagging: bootstrapping data + using the aggregate to make a decision\n",
    "- out of bag dataset(testing dataset): test which trees incorrectly predict --> out of bag error (the accuracy of the forest)\n",
    "<br>\n",
    "Flow:\n",
    "1. build a random forest\n",
    "2. estimate accuracy of the forest --> change number of variables used per step\n",
    "3. then repeat a bunch of times\n",
    "**Clustering**\n",
    "- fill in missing value by guess then refine guesses by running down the trees till the guess converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389f7e2-9848-4ef7-ad4c-13ebd64b9a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96624caa-b606-4f46-b219-05fb162ae79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bfbe2f2-31f6-4864-ba4f-80ed4f7b2d01",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5c43c-3c4c-41f4-9505-91cede6efa19",
   "metadata": {},
   "source": [
    "***\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
