{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542b1184-170c-4956-92c2-dc223fb9a7f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934aed75-029f-4cf1-9796-ecb105daaaa5",
   "metadata": {},
   "source": [
    "# Overview of scikit-learn\n",
    "<br>\n",
    "Scikit-learn (formerly scikits.learn and also known as sklearn) is an open source machine learning library for the Python programming language. It was initially developed by David Cournapeau in 2007. It features a large number of common algorithms,  including classification, regression, clustering and dimensionality reduction algorithms. It is designed to interoperate with the Python numerical and scientific libraries NumPy, Pandas, SciPy and Matplotlib. (wiki: https://en.wikipedia.org/wiki/Scikit-learn) <br>\n",
    "\n",
    "In general, **Machine Learning** is about creating models from data. Models are usully divided into 3 groups, namely supervised learning, unsupervised learning and reinforcement learning. Scikit-learn deals with the former two. It provides tools for model fitting, model selection and evaluation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd034cf",
   "metadata": {},
   "source": [
    "## So what is supervised learning and unsupervised learning? \n",
    "\n",
    "#### Supervised learning\n",
    "The data set comes with a column(s) of attribute(s) as target values that we want to predict. For example in the famous Iris dataset, \"species\" is the attribute we would want the model to predict. Depending on data type of the attribute(s), it is divided into two categories of algorithms, namely classification and regression. \n",
    "\n",
    "*  **Classification**: target attributes are in discrete form, with a limited number of categories. Aims to correctly label the target attributes. E.g., correctly label the speicies in Iris dataset\n",
    "*  **Regression**: target attributes are continuous variables, e.g. prediction of salmon's length as a function of its age and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab7546",
   "metadata": {},
   "source": [
    "#### Unsupervised learning\n",
    "The data set does not come with a column of attribute to be predicted. It aims to identify patterns from the data set. \n",
    "*   **Clustering**: aim to discover groups of similar examples within the data\n",
    "*   **Density estimation**: aim to determine the distribution of data within the sample \n",
    "*   **Dimentionality reduction**: project data from high-dimensional space down to 2/3D for the purpose of visualization, summarisation and feature selection. (scikit-learn website)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebc98c",
   "metadata": {},
   "source": [
    "## Scikit-learn divided their algorithms into 6 categories as follows:\n",
    "\n",
    "1. Classification\n",
    "2. Regression\n",
    "3. Clustering\n",
    "4. Dimensionality reduction\n",
    "5. Model selection \n",
    "    - Cross validation to check accuracy of supervised models\n",
    "    - Emsemble methods for combining the predictions of multiple supervised models\n",
    "6. Preprocessing \n",
    "    - Feature extraction to extract features from data to define the attributes in image and text data\n",
    "    - Feature selection to identify useful attributes to create supervised models\n",
    "\n",
    "ref:https://www.tutorialspoint.com/scikit_learn/scikit_learn_introduction.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee24a7f1",
   "metadata": {},
   "source": [
    "The steps in using sklearn as follows, \n",
    "1. arrange the data into features (x) and target(y, to be predicted),  \n",
    "2. a data set is then divided into training set and testing set, \n",
    "3. fit the model which is training \n",
    "5. predict() which is testing set, predicting target\n",
    "4. then evaluate its predictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5863af-27b0-429d-9645-5cd2c78dcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2fb80",
   "metadata": {},
   "source": [
    "# Decision Tree based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbb1bf",
   "metadata": {},
   "source": [
    "Interested in decision tree, it is easy enough to understand which we probably use it everyday to make decision, \n",
    "pick 3 algorithms which are related to DT, namely Gini impurity, Entropy or information gain, and bootstrap aggregating which is an ensemble method to enhance its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe7a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4ca7a41-131c-4f94-aee7-f9a9abdd2852",
   "metadata": {},
   "source": [
    "### How decision tree classifier works?\n",
    "https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bdc61-fb89-45e9-b3ff-b3c58cdbb89b",
   "metadata": {},
   "source": [
    "**Decision Tree** is under the umbrella of **Supervised Learning** algorithm, which is mainly used for classification problem. It works for both categorical **(Classification tree)** and continuous **(Regression tree)** dependent variables. It splits the population into two or more heterogeneous groups using various techniques like Gini index, Entropy. As well as identifying the most significant variable. <br>\n",
    "\n",
    "It can also be used in data exploration stage. For example, we are working on a problem where we have information available in hundreds of variables, there decision tree will help to identify most significant variable.<br>\n",
    "[ref]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57138c20-086c-45fa-a191-602e07f08abc",
   "metadata": {},
   "source": [
    "#### How does a tree based algorithms decide where to split\n",
    "The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees.\n",
    "\n",
    "Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n",
    "\n",
    "The algorithm selection is also based on type of target variables. Let’s look at the four most commonly used algorithms in decision tree:\n",
    "1. Gini\n",
    "2. Chi-square\n",
    "3. Information gain\n",
    "4. Reduction in Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4753b-c2bf-4afa-9a3a-1a8c7d225880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40229fa1-9cd4-42ed-b04f-c2c23e8d8371",
   "metadata": {},
   "source": [
    "### Gini Impurity (decision tree) (supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d0f95-9daf-4823-af95-2e680ec8a1ab",
   "metadata": {},
   "source": [
    "\n",
    "Random forest may be a flexible, easy to use machine learning algorithm that produces, even without hyper-parameter tuning, an excellent result most of the time.\n",
    "\n",
    "It’s also one among the foremost used algorithms, due to its simplicity and variety (it are often used for both classification and regression tasks).\n",
    "\n",
    "Random Forests are an ensemble learning method that is for performing classification, regression as well as other tasks through the construction of decision trees and providing the output as a class which is the mode or mean of the underlying individual trees.\n",
    "\n",
    "A Decision Tree Classifier functions by breaking down a dataset into smaller and smaller subsets based on different criteria. Different sorting criteria will be used to divide the dataset, with the number of examples getting smaller with every division.\n",
    "\n",
    "Once the network has divided the data down to one example, the example will be put into a class that corresponds to a key. When multiple random forest classifiers are linked together they are called Random Forest Classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05be674-f014-4810-a13e-03d804a8e34c",
   "metadata": {},
   "source": [
    "**Classification Tree**\n",
    "- make tree for each variable, check and compare impurity among variables (Gini impurity, imformation gain, entropy)\n",
    "- Total Gini impurity = weighted average of gini impurities for the leaves\n",
    "- a bit more calculation if variable is numeric\n",
    "- variable with lowest impurity at the top of the tree, then calcualtion of Gini Imppurity starts again for the remaining variables as nodes, until no more split --> as leaf and categorize/classify them\n",
    "**confidence in classification and overfit**\n",
    "- Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa41bd-9e0b-41ca-b80d-73cb7a4a2a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84badc65-3f64-4625-a167-af798fd070d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965a80e-7e7f-4d77-a4cf-c84b50236f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddb54df-5532-475d-886d-ca0677c850bc",
   "metadata": {},
   "source": [
    "### Bootstrap aggregating/baggin algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b32f2-6a5f-450a-af62-e059aa581379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a7f7d-ea6c-44d9-bb5b-cfe2e2f8a66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a465bc2-5b9a-4496-8a00-0f42bd09243d",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "https://builtin.com/data-science/random-forest-algorithm\n",
    "\n",
    "https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "\n",
    "https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/\n",
    "\n",
    "\n",
    "It is essentially an ensemble of decision trees. While decision trees are better at classification, random forest is better at prediction. \n",
    "multiple decision trees, usually trained with the \"bagging\" method which is generally combination learning models increases the overall result in terms of accuracy and stability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f5e52-0399-41ec-aa86-2a039fe7f983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c01237df-0361-4f55-ad47-dab6589ae007",
   "metadata": {},
   "source": [
    "### Pruning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f14349-6c9d-43f6-b25a-dc4f232a5d52",
   "metadata": {},
   "source": [
    "#### random forest\n",
    "- because DT is easy to create but inaccurate to classify\n",
    "- bootstrapped dataset; consider a random subset of variables at each step; then repeat --> a wide variety of trees (the forest)\n",
    "- prediction, run data on all the trees in the forest, see which option received more votes\n",
    "- bagging: bootstrapping data + using the aggregate to make a decision\n",
    "- out of bag dataset(testing dataset): test which trees incorrectly predict --> out of bag error (the accuracy of the forest)\n",
    "<br>\n",
    "Flow:\n",
    "1. build a random forest\n",
    "2. estimate accuracy of the forest --> change number of variables used per step\n",
    "3. then repeat a bunch of times\n",
    "**Clustering**\n",
    "- fill in missing value by guess then refine guesses by running down the trees till the guess converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389f7e2-9848-4ef7-ad4c-13ebd64b9a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96624caa-b606-4f46-b219-05fb162ae79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bfbe2f2-31f6-4864-ba4f-80ed4f7b2d01",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5c43c-3c4c-41f4-9505-91cede6efa19",
   "metadata": {},
   "source": [
    "***\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
